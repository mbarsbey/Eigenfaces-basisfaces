{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variational inference description and implementation\n",
    "\n",
    "In Bayesian computation we are most likely interested in making inference about hidden variables. From frequentist perspective we are interested in parameters of the population that most likely have generated the sample we observe. In Bayesian perspective, on the other hand, we are interested in what we know about possible values of parameters given the fact that we observe the data at hand.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(Z|X) &= \\frac{P(X|Z)P(Z)}{P(X)}\n",
    "        &= \\frac{P(X|Z)P(Z)}{\\int_Z P(X|Z)P(Z)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Usually enough, we cannot compute the sum in the denominator easily. Variational inference is a method  for approximating the posterior of the hidden variables. The way we do it is we define a distribution Q with parameters $\\phi$ and we try to make this variational distribution as close as possible to the actual posterior of the hidden variables by finding the optimal values of $\\phi$.\n",
    "\n",
    "The difference between two distributions is measured by KL divergence. Thus we are trying to minimize\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "KL(q(z|\\phi)|p(z|x)) &= \\langle \\log q(z|\\phi) - \\log p(z|x) \\rangle_{q(z|\\phi)} \\\\\n",
    "                     &= \\langle \\log q(z|\\phi) - \\log p(z, x) + \\log p(x) \\rangle_{q(z|\\phi)} \\\\\n",
    "                     &= \\langle \\log q(z|\\phi) - \\log p(z, x) \\rangle_{q(z|\\phi)} + \\log p(x)  \\geq 0 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "since KL divergence is always nonnegative. Since \\log p(x) does not depend on q, we aim to maximize $\\langle \\log q(z|\\phi) - \\log p(z, x) \\rangle$ or minimize its negative. This value is called evidence lower bound (ELBO) since:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\log p(x)  \\geq \\langle - \\log q(z|\\phi) + \\log p(z, x)  \\rangle_{q(z|x)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let us demonstrate this idea on a simple example.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\pi &\\sim B(\\alpha, \\beta) \\\\\n",
    "x_i &\\sim BE(\\pi)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let our variational distribution be $q(z|\\phi) = q(\\pi|a,b) = B(a, b)$. Then ELBO is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L} &= \\langle -(a-1)\\log \\pi - (b-1)\\log (1- \\pi) + \\log B(a, b) +(\\alpha -1)\\log \\pi + (\\beta-1)\\log (1-\\pi) - \\log B(\\alpha, \\beta) + \\sum_i x_i \\log \\pi + (1 - x_i) \\log (1-\\pi)    \\rangle_{q(\\pi|a,b)} \\\\\n",
    "            &= \\langle (\\alpha - a + \\sum_i x_i)\\log \\pi + (\\beta - b + \\sum_i (1 - x_i))\\log (1- \\pi) \\rangle_{q(\\pi|a,b)} + \\log B(a, b) - \\log B(\\alpha, \\beta)  \\\\\n",
    "            &= (\\alpha - a + \\sum_i x_i)\\langle \\log \\pi \\rangle_{q(\\pi|a,b)} + (\\beta - b + \\sum_i (1 - x_i))\\langle\\log (1- \\pi)\\rangle_{q(\\pi|a,b)}  + \\log B(a, b) - \\log B(\\alpha, \\beta)  \\\\\n",
    "            &= (\\alpha - a + \\sum_i x_i)(\\psi(a)-\\psi(a + b)) + (\\beta - b + \\sum_i (1 - x_i))(\\psi(b)-\\psi(a + b))  + \\log B(a, b) - \\log B(\\alpha, \\beta)  \\\\\n",
    "            &= (\\alpha - a + \\sum_i x_i)(\\psi(a)-\\psi(a + b)) + (\\beta - b + \\sum_i (1 - x_i))(\\psi(b)-\\psi(a + b))  + \\log B(a, b) - \\log B(\\alpha, \\beta)  \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we take the derivative of the ELBO with respect to a and b:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial a} &= (\\alpha - a + \\sum_i x_i)(\\psi(a) - \\psi(a + b))' - (\\beta - b + \\sum_i (1 - x_i))\\psi'(a + b)\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b} &= - (\\alpha - a + \\sum_i x_i)\\psi'(a + b) - (\\beta - b + \\sum_i (1 - x_i))(\\psi(a) - \\psi(a + b))'\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Both equations equal 0 when:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a &= \\alpha + \\sum_i x_i \\\\\n",
    "b &= \\beta + \\sum_i (1 - x_i)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thus, \n",
    "$$\n",
    "q(\\pi|a,b) = B(\\alpha + \\sum_i x_i, \\beta + \\sum_i (1 - x_i))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi = 0.8364746676128703\n"
     ]
    }
   ],
   "source": [
    "alpha_par = 1\n",
    "beta_par = 1\n",
    "b = beta(alpha_par, beta_par)\n",
    "pi = b.rvs()\n",
    "print(\"pi = {}\".format(pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_beta(b):\n",
    "    x = np.linspace(0,1,100)\n",
    "    y = b.pdf(x)\n",
    "    plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
